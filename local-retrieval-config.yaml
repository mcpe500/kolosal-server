server:
  port: 8080
  host: 0.0.0.0
  idle_timeout: 300
  allow_public_access: false
  allow_internet_access: false

logging:
  level: INFO
  file: ""
  access_log: false
  quiet_mode: false
  show_request_details: true

database:
  vector_database: faiss
  retrieval_embedding_model: qwen3-embedding-0.6b
  faiss:
    index_type: Flat
    index_path: ./data/faiss_qwen06b_index
    dimensions: 1536
    normalize_vectors: true
    metric_type: IP
    nlist: 100
    nprobe: 10
    use_gpu: false
    gpu_device: 0

auth:
  enabled: false

search:
  enabled: false

models:
  - id: qwen3-embedding-0.6b
    path: https://huggingface.co/kolosal/qwen3-embedding-0.6b/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf
    type: embedding
    load_immediately: true
    main_gpu_id: 0
    inference_engine: llama-cpu
    load_params:
      n_ctx: 512
      n_keep: 0
      use_mmap: true
      use_mlock: false
      n_parallel: 2
      cont_batching: true
      warmup: false
      n_gpu_layers: 100
      n_batch: 256
      n_ubatch: 64

inference_engines:
  - name: llama-cpu
    library_path: ./build/Release/llama-cpu.dll
    version: 1.0.0
    description: CPU-based inference engine for LLaMA models
    load_on_startup: true

default_inference_engine: llama-cpu

features:
  health_check: true
  metrics: true
